{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a13fc0",
   "metadata": {},
   "source": [
    "# Foodie Recommender Data Model (V0.1)\n",
    "We'll choose a set of starter features and attempt to train a two-towered recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7cb29",
   "metadata": {},
   "source": [
    "## Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24be2c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "from pydantic import BaseModel, validator\n",
    "import os\n",
    "from typing import Any, List\n",
    "\n",
    "import json\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "\n",
    "genres = {}\n",
    "\n",
    "\n",
    "\n",
    "def save_model_data(model: BaseModel, filename: str) -> str:\n",
    "    with open(filename, 'w+') as file:\n",
    "        json.dump(model.json(), file)\n",
    "    return filename\n",
    "\n",
    "\n",
    "\n",
    "def create_empty_genres_file(filepath: str) -> None:\n",
    "    with open(filepath, 'w+') as file:\n",
    "        json.dump({}, file)\n",
    "    return {}\n",
    "        \n",
    "        \n",
    "\n",
    "def load_or_create_genres(genres_file='genres.json') -> dict:\n",
    "    if os.path.isfile(genres_file):\n",
    "        with open(genres_file, 'r') as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        print('file not found', genres_file)\n",
    "        return create_empty_genres_file(genres_file)\n",
    "\n",
    "\n",
    "    \n",
    "def save_genres(genres: dict, genres_file='genres.json') -> dict:\n",
    "    with open(genres_file, 'w+') as file:\n",
    "        json.dump(genres, file)\n",
    "    return genres\n",
    "\n",
    "    \n",
    "        \n",
    "def add_element_to_genres(element: str) -> int:\n",
    "    genres = load_or_create_genres()\n",
    "    dict_len = len(genres)\n",
    "    if element not in genres:\n",
    "        genres[element] = dict_len + 1\n",
    "        genres = save_genres(genres)\n",
    "    return genres[element]\n",
    "        \n",
    "        \n",
    "        \n",
    "def map_genre(genre_list: list) -> list:\n",
    "    tmp = []\n",
    "    for element in genre_list:\n",
    "        tmp.append(add_element_to_genres(element))\n",
    "    return tmp\n",
    "        \n",
    "\n",
    "    \n",
    "class RestaurantUser(BaseModel):\n",
    "    user_birth_date: int\n",
    "    user_genres: List[Any] = [0]\n",
    "    user_id: int\n",
    "    user_occupation: str\n",
    "    user_gender: bool  # 0: male, 1: female\n",
    "    user_zip_code: int\n",
    "    \n",
    "    \n",
    "    @validator('user_genres')\n",
    "    def index_or_add(cls, v):\n",
    "        assert len(v) > 0, 'Must provide list of genre > 0'\n",
    "        return map_genre(v)\n",
    "    \n",
    "    def save(self, prefix='user') -> None:\n",
    "        name = prefix + f\"_{self.user_id}.json\"\n",
    "        return save_model_data(self, name)\n",
    "\n",
    "    \n",
    "        \n",
    "class Restaurant(BaseModel):\n",
    "    restaurant_id: int\n",
    "    restaurant_title: str\n",
    "    restaurant_genres: List[Any]\n",
    "    restaurant_zip_code: int\n",
    "   \n",
    "    @validator('restaurant_genres')\n",
    "    def index_or_add(cls, v):\n",
    "        assert len(v) > 0, 'Must provide list of genre > 0'\n",
    "        return map_genre(v)\n",
    "        \n",
    "    def save(self, prefix='restaurant') -> str:\n",
    "        name = prefix + f\"_{self.restaurant_id}.json\"\n",
    "        return save_model_data(self, name)\n",
    "    \n",
    "    \n",
    "    \n",
    "class RestaurantRating(BaseModel):\n",
    "    user: RestaurantUser\n",
    "    restaurant: Restaurant\n",
    "    timestamp: int  # Converting all date/time to posix integer\n",
    "    restaurant_rating: int\n",
    "    \n",
    "    @validator('timestamp')\n",
    "    def convert_valid_time(cls, v: str):\n",
    "        return v\n",
    "    \n",
    "    def save(self, prefix='rating') -> str:\n",
    "        name = prefix + f\"_{self.restaurant}_{self.user}_{self.timestamp}.json\"\n",
    "        return save_model_data(self, name)\n",
    "    \n",
    "    def flatten(self) -> dict:\n",
    "        tmp = {\n",
    "            **self.user.dict(), **self.restaurant.dict(),\n",
    "            'timestamp': self.timestamp,\n",
    "            'restaurant_rating': self.restaurant_rating,\n",
    "        }\n",
    "        return tmp\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106f9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test user\n",
    "user_profile = {\n",
    "    \"user_birth_date\": 20220101,\n",
    "    \"user_genres\": ['vegetarian', 'thai'],\n",
    "    \"user_id\": 1001,\n",
    "    \"user_occupation\": \"student\",\n",
    "    \"user_gender\": 0,  # 0: male, 1: female\n",
    "    \"user_zip_code\": 84104,\n",
    "}\n",
    "\n",
    "user = RestaurantUser(**user_profile)\n",
    "\n",
    "# Create a test restaurant\n",
    "restaurant_profile = {\n",
    "    \"restaurant_id\": 1,\n",
    "    \"restaurant_title\": \"skinnyfats\",\n",
    "    \"restaurant_genres\": ['vegetarian', 'thai', 'healthy', 'fried'],\n",
    "    \"restaurant_zip_code\": 84104,\n",
    "}\n",
    "\n",
    "restaurant = Restaurant(**restaurant_profile)\n",
    "\n",
    "# Create a test rating\n",
    "rating_profile = {\n",
    "    \"user\": user,\n",
    "    \"restaurant\": restaurant,\n",
    "    \"timestamp\": 202201012200,  # Converting all date/time to posix integer\n",
    "    \"restaurant_rating\": 10,\n",
    "}\n",
    "\n",
    "rating = RestaurantRating(**rating_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004c7e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_birth_date': 20220101,\n",
       " 'user_genres': [1, 2],\n",
       " 'user_id': 1001,\n",
       " 'user_occupation': 'student',\n",
       " 'user_gender': False,\n",
       " 'user_zip_code': 84104,\n",
       " 'restaurant_id': 1,\n",
       " 'restaurant_title': 'skinnyfats',\n",
       " 'restaurant_genres': [1, 2, 3, 4],\n",
       " 'restaurant_zip_code': 84104,\n",
       " 'timestamp': 202201012200,\n",
       " 'restaurant_rating': 10}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The rating object contains user and restaurant models \n",
    "# Here, we implement a helper fn flatten to make it an easier document to deal with\n",
    "rating.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60444f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_birth_date': 20220101,\n",
       " 'user_genres': [1, 2],\n",
       " 'user_id': 1001,\n",
       " 'user_occupation': 'student',\n",
       " 'user_gender': False,\n",
       " 'user_zip_code': 84104}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our user model can output a dictionary as well with a direct call to the dict() method\n",
    "user.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a35e82",
   "metadata": {},
   "source": [
    "## Convert to Tensorflow Dataset\n",
    "We'll now arbitrarily copy the data and create Tensorflow datasets to train new embeddigns models with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b8531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc08d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll convert the dict values to np arrays first\n",
    "\n",
    "def values_to_array(input_dict: dict) -> np.array:\n",
    "    tmp = {}\n",
    "    for key in input_dict:\n",
    "        tmp[key] = np.array([input_dict[key]])\n",
    "    return tmp\n",
    "\n",
    "# Once the data is in array format, elementwise concatenation will emulate a row\n",
    "def concatenate_dicts(dicts: List[dict]) -> dict:\n",
    "    \n",
    "    def _fetch_values(dicts: list, key: str) -> list:\n",
    "        return tuple([d[key] for d in dicts])\n",
    "    \n",
    "    parent = dicts[0]\n",
    "    tmp = {}\n",
    "    for key in parent:\n",
    "        tmp[key] = np.concatenate(_fetch_values(dicts, key))\n",
    "    return tmp\n",
    "\n",
    "# Convert to tensforflow dataset\n",
    "def convert_dict_to_tflow(d: dict) -> tf.data.Dataset:\n",
    "    return tf.data.Dataset.from_tensor_slices(d)\n",
    "\n",
    "        \n",
    "arrayed_user = values_to_array(user.dict())\n",
    "users = convert_dict_to_tflow(\n",
    "    concatenate_dicts([arrayed_user]*10000)\n",
    ")\n",
    "\n",
    "arrayed_rating = values_to_array(rating.flatten())\n",
    "ratings = convert_dict_to_tflow(\n",
    "    concatenate_dicts([arrayed_rating]*10000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5abe0d",
   "metadata": {},
   "source": [
    "## Data Manipulation for Embedding\n",
    "We still need to generate embeddings for each tower.  We can use similar transformations as in the base tutorial for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae6bb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'restaurant_genres': array([1, 2, 3, 4]),\n",
      " 'restaurant_id': 1,\n",
      " 'restaurant_rating': 10,\n",
      " 'restaurant_title': b'skinnyfats',\n",
      " 'restaurant_zip_code': 84104,\n",
      " 'timestamp': 202201012200,\n",
      " 'user_birth_date': 20220101,\n",
      " 'user_gender': False,\n",
      " 'user_genres': array([1, 2]),\n",
      " 'user_id': 1001,\n",
      " 'user_occupation': b'student',\n",
      " 'user_zip_code': 84104}\n",
      "{'user_birth_date': 20220101,\n",
      " 'user_gender': False,\n",
      " 'user_genres': array([1, 2]),\n",
      " 'user_id': 1001,\n",
      " 'user_occupation': b'student',\n",
      " 'user_zip_code': 84104}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)\n",
    "for u in users.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52ca6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map categorical features to embeddings for models\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    # Create a layer that turns strings into integer indices.\n",
    "    if dtype == 'string':\n",
    "        index = tf.keras.layers.StringLookup(max_tokens=max_tokens)\n",
    "        # Otherwise, create a layer that turns integer values into integer indices.\n",
    "    else:\n",
    "        index = tf.keras.layers.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "    # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "    feature_ds = dataset.map(lambda x: x[name])\n",
    "\n",
    "    # Learn the set of possible values and assign them a fixed integer index.\n",
    "    index.adapt(feature_ds)\n",
    "\n",
    "    # Encode the integer indices.\n",
    "    encoder = tf.keras.layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "    # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "    # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "\n",
    "def get_text_tokenization_layer(name, dataset, max_features=1000, max_len=12):\n",
    "    # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "    feature_ds = dataset.map(lambda x: x[name])\n",
    "    \n",
    "    # Specify a vectorization layer\n",
    "    vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=max_features,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_len)\n",
    "    \n",
    "    vectorize_layer.adapt(feature_ds.batch(64))\n",
    "    \n",
    "    # Create a model that can use the layer on the feature dataset\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Explicit input layer\n",
    "    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "    # vectorization layer\n",
    "    model.add(vectorize_layer)\n",
    "    \n",
    "    # Apply the model to indices\n",
    "    return lambda feature: model.predict(feature)\n",
    "\n",
    "    \n",
    "user_occ_layer = get_category_encoding_layer('user_occupation', users, 'string')\n",
    "rest_title_layer = get_text_tokenization_layer('restaurant_title', ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db82528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Transform datasets\n",
    "\n",
    "users_encoded = users.map(\n",
    "    lambda x: (\n",
    "        x['user_birth_date'],\n",
    "        x['user_gender'],\n",
    "        x['user_genres'],\n",
    "        x['user_id'],\n",
    "        user_occ_layer(x['user_occupation']),\n",
    "        x['user_zip_code'],\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "ratings_rest_titles = tf.data.Dataset.from_tensor_slices(\n",
    "    {'restaurant_title': rest_title_layer(ratings.batch(1000).map(lambda x: x['restaurant_title']))}\n",
    ")\n",
    "\n",
    "ratings_encoded = ratings.map(\n",
    "    lambda x: (\n",
    "        x['restaurant_genres'],\n",
    "        x['restaurant_id'],\n",
    "        x['restaurant_rating'],\n",
    "        x['restaurant_zip_code'],\n",
    "        x['timestamp'],\n",
    "        x['user_birth_date'],\n",
    "        x['user_gender'],\n",
    "        x['user_genres'],\n",
    "        x['user_id'],\n",
    "        user_occ_layer(x['user_occupation']),\n",
    "        x['user_zip_code'],\n",
    "    )\n",
    ")\n",
    "\n",
    "ratings_encoded = tf.data.Dataset.zip((ratings_encoded, ratings_rest_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d606c94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20220101, False, array([1, 2]), 1001, array([0., 1.], dtype=float32), 84104),\n",
       " ((array([1, 2, 3, 4]),\n",
       "   1,\n",
       "   10,\n",
       "   84104,\n",
       "   202201012200,\n",
       "   20220101,\n",
       "   False,\n",
       "   array([1, 2]),\n",
       "   1001,\n",
       "   array([0., 1.], dtype=float32),\n",
       "   84104),\n",
       "  {'restaurant_title': array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)}))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(users_encoded.take(1).as_numpy_iterator()), next(ratings_encoded.take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62811bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layers\n",
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a1e86",
   "metadata": {},
   "source": [
    "### Query Tower\n",
    "\n",
    "Given a user (or set of users), yield embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1991191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(7, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db72b65c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\u1377381\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\u1377381\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\u1377381\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\u1377381\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in predict_step\n        x, _, _ = data_adapter.unpack_x_y_sample_weight(data)\n    File \"C:\\Users\\u1377381\\Miniconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1579, in unpack_x_y_sample_weight\n        raise ValueError(error_msg)\n\n    ValueError: Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`, found: (<tf.Tensor 'IteratorGetNext:0' shape=() dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=() dtype=bool>, <tf.Tensor 'IteratorGetNext:2' shape=(2,) dtype=int32>, <tf.Tensor 'IteratorGetNext:3' shape=() dtype=int32>, <tf.Tensor 'IteratorGetNext:4' shape=(2,) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=() dtype=int32>)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m user_model\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m output_array \u001b[38;5;241m=\u001b[39m \u001b[43muser_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43musers_encoded\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filejufm9l2j.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\u1377381\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\u1377381\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\u1377381\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\u1377381\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in predict_step\n        x, _, _ = data_adapter.unpack_x_y_sample_weight(data)\n    File \"C:\\Users\\u1377381\\Miniconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1579, in unpack_x_y_sample_weight\n        raise ValueError(error_msg)\n\n    ValueError: Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`, found: (<tf.Tensor 'IteratorGetNext:0' shape=() dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=() dtype=bool>, <tf.Tensor 'IteratorGetNext:2' shape=(2,) dtype=int32>, <tf.Tensor 'IteratorGetNext:3' shape=() dtype=int32>, <tf.Tensor 'IteratorGetNext:4' shape=(2,) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=() dtype=int32>)\n"
     ]
    }
   ],
   "source": [
    "user_model.compile('rmsprop', 'mse')\n",
    "output_array = user_model.predict(users_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "401cc54e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "call() takes from 2 to 4 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m tfrs\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mFactorizedTopK(\n\u001b[1;32m----> 2\u001b[0m     candidates\u001b[38;5;241m=\u001b[39m\u001b[43musers_encoded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m task \u001b[38;5;241m=\u001b[39m tfrs\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mRetrieval(\n\u001b[0;32m      6\u001b[0m     metrics\u001b[38;5;241m=\u001b[39mmetrics\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2048\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2045\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DEBUG_MODE:\n\u001b[0;32m   2046\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2047\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2048\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2049\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2050\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[0;32m   2051\u001b[0m       \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2052\u001b[0m       map_func,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2055\u001b[0m       preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2056\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5243\u001b[0m, in \u001b[0;36mMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m   5242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality \u001b[38;5;241m=\u001b[39m preserve_cardinality\n\u001b[1;32m-> 5243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5247\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m   5249\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmap_dataset(\n\u001b[0;32m   5250\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   5251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5254\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[0;32m   5255\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    265\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    269\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2567\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2559\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   2560\u001b[0m \n\u001b[0;32m   2561\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2565\u001b[0m \u001b[38;5;124;03m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2567\u001b[0m   graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_concrete_function_garbage_collected(\n\u001b[0;32m   2568\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2569\u001b[0m   graph_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2570\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2533\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2531\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 2533\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2534\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m   2535\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[0;32m   2536\u001b[0m       graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2711\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2708\u001b[0m   cache_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mgeneralize(cache_key)\n\u001b[0;32m   2709\u001b[0m   (args, kwargs) \u001b[38;5;241m=\u001b[39m cache_key\u001b[38;5;241m.\u001b[39m_placeholder_value()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 2711\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2712\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[0;32m   2713\u001b[0m                          graph_function)\n\u001b[0;32m   2715\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2627\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2622\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2623\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   2624\u001b[0m ]\n\u001b[0;32m   2625\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   2626\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 2627\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2635\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   2637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   2638\u001b[0m     spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   2639\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   2640\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   2641\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   2642\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   2643\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1141\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1139\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1141\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m   1146\u001b[0m     convert, func_outputs, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:248\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;129m@eager_function\u001b[39m\u001b[38;5;241m.\u001b[39mdefun_with_attributes(\n\u001b[0;32m    243\u001b[0m     input_signature\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_specs(\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_structure),\n\u001b[0;32m    245\u001b[0m     autograph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    246\u001b[0m     attributes\u001b[38;5;241m=\u001b[39mdefun_kwargs)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:177\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[0;32m    176\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[1;32m--> 177\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n\u001b[0;32m    179\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ret)\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    455\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 458\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: call() takes from 2 to 4 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "    candidates=users_encoded.batch(128).map(user_model)\n",
    ")\n",
    "\n",
    "task = tfrs.tasks.Retrieval(\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f782c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
